{"cells":[{"metadata":{"trusted":true,"_uuid":"a3e9c6da6470fe1ec0596ebba0011b76a25158cd"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5289aabd1ee78b49d3ba09806b6f7f52b85b82c7"},"cell_type":"code","source":"import theano\n\n\"\"\"\nThese are the functions that we will require if we want to compute the predictions\nand Elo ratings for teams by hand (traditional method). You can find these equations\nand their explanation here: http://dataskeptic.com/blog/methods/2017/calculating-an-elo-rating\n\"\"\"\n\ndef Logistic_Rating(rating):\n    return 10.0**(rating/400)\n\ndef Expected(rating1, rating2):\n    \"\"\"\n    These should be Logistic Ratings\n    \"\"\"\n    log_rating1 = Logistic_Rating(rating1)\n    log_rating2 = Logistic_Rating(rating2)\n    return log_rating1/(log_rating1+log_rating2)\n\ndef Update_Rating(rating1, K, S, E):\n    \"\"\"\n    rating1: rating should be the r\n    K : Scaling factor between 1,99\n    S : Outcome of game [0,1]\n    E : Expected Outcome of the game\n    \"\"\"\n    return rating1 + K*(S-E)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8054d7d3ad9b418317f1112b1fe3a4e9a5cf62cb"},"cell_type":"code","source":"\"\"\"\nLoad in the data set of teams and their overall win/loss fraction for a match.\nEach match consists of a number of games, and the overall fraction represents the\nwin rate of the first team. If a match between Team A and Team B resulted in\n1 win for Team A and 3 wins for Team B, the data would look like this:\n====================\nTeam A, 0.25, Team B\n\"\"\"\n\nregular_season = pd.read_csv('../input/regular_season_frac_score.csv')\nregular_season.head()                             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc10122e006b01e728553655dac429ef7d5b790e"},"cell_type":"code","source":"\"\"\"\nFor each team, let us make an index for them.\n\"\"\"\n\nteams = regular_season.Team1.unique() \nn_teams = len(teams)\nteams = pd.DataFrame({\"index\":range(len(teams)), \"team\": teams})\nteams.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d26cb067495667d450b8e3eeee394624778e4b6"},"cell_type":"code","source":"\"\"\"\nJoin the team index with the record values.\n\"\"\"\n\nregular_season = pd.merge(regular_season,\\\n                    teams.rename(index=str,\\\n                    columns={\"team\": \"Team1\", \"index\": \"team1_index\"}),\n                    on = ['Team1'] )\n\nregular_season = pd.merge(regular_season,\\\n                    teams.rename(index=str,\\\n                    columns={\"team\": \"Team2\", \"index\": \"team2_index\"}),\n                    on = ['Team2'] )\n\nregular_season = regular_season.drop(['Team1','Team2'], axis=1)\n\nregular_season.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c14ffba49ab5bafd0a914476d193a08bfed11ec6"},"cell_type":"code","source":"import theano.tensor as T\n\n\"\"\"\nIn PyMC3, we can enforce the model data to be in this \nshared tensor format Theano provides. That way we can do\ntrain and evaluation splits effectively.\n\"\"\"\n\nwinner_index = theano.shared(np.array(regular_season['team1_index']))\nloser_index = theano.shared(np.array(regular_season['team2_index']))\nmodel_output = theano.shared(np.array(regular_season['Record']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75be142baf468e99200ab4abcd89b28b92b6bb8c"},"cell_type":"code","source":"import pymc3 as pm\n\nprint('Running on PyMC3 v{}'.format(pm.__version__))\n\nelo = pm.Model()\n\nwith elo:\n    \"\"\"\n    We are going to model the Elo rating of each team as a Normal Distribution \n    centered around 2000. We use a gamma distribution , but a HalfCauchy \n    can be used as well.\n    \"\"\"\n    sigma = pm.Gamma('noise', alpha=10.0, beta=1)\n    elo_team = pm.Normal(\"elo_team\", mu = 2000.0, sd=sigma, shape=n_teams)\n    \n    \"\"\"\n    We can convert the Elo ratings to a logistic rating. This enables us to\n    make predictions on individual matchup outcomes.\n    \"\"\"\n    log_rating2 = 10.0**(elo_team[loser_index]/400.0)\n    log_rating1 = 10.0**(elo_team[winner_index]/400.0)\n    E = log_rating1/(log_rating1+log_rating2) # Expected value\n    error =  pm.HalfCauchy('error', beta=1.0)\n    \n    \"\"\"\n    The Expectation value is a value between 0 and 1, with\n    0 predicting that Team 1 has a 0% chance of winning the matchup.\n    \"\"\"    \n    out = pm.Normal('out', mu=E, sd = error, observed=model_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80be5b686b12b96c82c7bfdf1d3443009421ba0"},"cell_type":"code","source":"\"\"\"\nYou can choose to use ADVI or NUTS sampler. As this \nis such a small dataset, the NUTS works fine.\n\"\"\"\n\nuse_advi = False\nwith elo:\n    if use_advi:\n        inference = pm.ADVI()\n        advi_approx = pm.fit(n=100000, method=inference)\n    else:\n        trace = pm.sample(32000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc2b2b65ca824173140c03c3b835623c825eba1"},"cell_type":"code","source":"#Now we sample from our approximation in order to get a similar trace\nif use_advi:\n    trace = advi_approx.sample(10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eecd5fd8d811c6d95f9e3d3fbb3d580442dd41b5"},"cell_type":"code","source":"\"\"\"\nWhat do our ratings look like? How are they distributed?\n\n\"\"\"\n\npm.traceplot(trace);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c5eb969958e7751b74453878966bd728720c730"},"cell_type":"code","source":"pm.summary(trace)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d94e934b397471f3771a9730d493f9c0ddca944e"},"cell_type":"code","source":"\"\"\"\nMap our ELO rating to various teams.\n\"\"\"\n\nwins = wins.sort_values(by='index',ascending=True)\nwins['Elo'] = trace.elo_team.mean(axis=0)\nwins = wins.sort_values(by='Record',ascending=False)\nwins.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa02e4eca0f5f5835234922f746f109013e2990f"},"cell_type":"code","source":"\"\"\"\nTwo helper functions to either make prediction probabilities for some inputs,\nor to score the model on a set of target values.\n\nYou can think of these utilities as sampling from the _distribution_ of \npossible Elo ratings for each team, instead of a single value. If our model \nis very uncertain of a rating for a given team, their prediction value will be\nreflective of that.\n\"\"\"\n\ndef make_preds(trace,model_name):\n    ppc = pm.sample_posterior_predictive(trace, model=model_name, samples=500)\n    return ppc['out'].mean(axis=0)\n\ndef scoreModel(trace,y,model_name):\n    ppc = pm.sample_posterior_predictive(trace, model=model_name, samples=2000)\n    pred = ppc['out'].mean(axis=0)\n    print (\"RMSE: %0.3f\" %(  np.sqrt(mean_squared_error([round(rec) for rec in y],[round(x) for x in pred]))))\n    print (\"ROC AUC: %0.3f\" %(roc_auc_score([round(rec) for rec in y], pred)))\n    print (\"Accuracy: %0.3f\" %(accuracy_score([round(rec) for rec in y],[round(x) for x in pred]))) \n\nscoreModel(trace,regular_season['Record'],elo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae3cd96d62e017f8d3369a8a1b960aeecbb5cc69"},"cell_type":"code","source":"\"\"\"\nUnfortunately there are very few playoff matches,\nso our model will have to be very good on a small \nnumber of samples.\n\"\"\"\n\nplayoffs = pd.read_csv('../input/playoffs_frac_score.csv')\nprint (playoffs.count()[0])\nplayoffs.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d202bb1549b1dc458429e433cd7c48b4ef9205"},"cell_type":"code","source":"playoffs = pd.merge(playoffs,\\\n                    teams.rename(index=str,\\\n                    columns={\"team\": \"Team1\", \"index\": \"team1_index\"}),\n                    on = ['Team1'] )\n\nplayoffs = pd.merge(playoffs,\\\n                    teams.rename(index=str,\\\n                    columns={\"team\": \"Team2\", \"index\": \"team2_index\"}),\n                    on = ['Team2'] )\n\nplayoffs = playoffs.drop(['Team1','Team2'], axis=1)\nplayoffs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aef3d5f6f4ae5a852055d5380ec20dec4c0612e7"},"cell_type":"code","source":"\"\"\"\nInitialize our evaluation data set in the shared tensor.\n\"\"\"\n\nwinner_index.set_value(np.array(playoffs['team1_index']))\nloser_index.set_value(np.array(playoffs['team2_index']))\nmodel_output.set_value(np.array(playoffs['Record']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33b74a09978c2b877e77a743c72c7d86b1b3393b"},"cell_type":"code","source":"\"\"\"\nOur model does not do that well. Our RMSE of our \nExpected value is pretty poor, and our ROC is just above \nrandom guessing. \n\"\"\"\n\nscoreModel(trace,playoffs['Record'],elo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5952c6e0c82ff169bc168463f9fc3cdab3474054"},"cell_type":"code","source":"\"\"\"\nIf we examine our predictions in more detail, we can see that \nour model is not confident at all. Most matches end fairly \ndecisively, but our model generally only gives 50% odds\nto any given outcome.\n\"\"\"\n\nplayoffs['predictions'] = make_preds(trace,elo)\nplayoffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8f08a4cdc25e2d4bf0d952ac448f0256d532dc7"},"cell_type":"code","source":"\"\"\"\nUsing the mean value of each Elo rating that our model gives us,\nwe can make raw predictions as well.\n\"\"\"\n\nplayoffs = pd.merge(playoffs, wins[['index','Elo']].rename(index=str,columns={'index':\"team1_index\"}), on=['team1_index'],how='inner')\nplayoffs = playoffs.rename(index=str,columns={'Elo': 'team1_elo'})\nplayoffs = pd.merge(playoffs, wins[['index','Elo']].rename(index=str,columns={'index':\"team2_index\"}), on=['team2_index'],how='inner')\nplayoffs = playoffs.rename(index=str,columns={'Elo': 'team2_elo'})\nplayoffs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba8bc7a36f0f2346dc162ed635b920459857083"},"cell_type":"code","source":"\"\"\"\nAs expected, this does not affect our metrics to any significance.\n\"\"\"\n\nplayoffs['raw_expected'] = Expected(playoffs['team1_elo'],playoffs['team2_elo'])\nplayoffs.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52e068e55e62c7432d545eebcf720143dfec5ccc"},"cell_type":"code","source":"print (\"ROC AUC: %0.3f\" %(roc_auc_score([round(rec) for rec in playoffs['Record']], playoffs['raw_expected'])))\nprint (\"Accuracy: %0.3f\" %(accuracy_score([round(rec) for rec in playoffs['Record']],[round(x) for x in playoffs['raw_expected']]))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"051fd608ffe64b65bda4b28fea4ebcc3138270e1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}